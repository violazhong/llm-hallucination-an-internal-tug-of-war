# LLM Hallucination: An Internal Tug of War

This repository contains the code and findings from a research sprint investigating a surprising "Confidence Bias" in Llama-2-13b-chat-hf. My experiment results reveal a specific, biased circuit that provides a direct, mechanistic explanation for the "incentivized guessing" phenomenon described by the concurrent OpenAI paper [Why Language Models Hallucinate](https://arxiv.org/abs/2509.04664), suggesting a root cause for confident hallucination.